k-Nearest Neighbours is another classification framework. 

Linear regression, as well as neural networks, are parametric models - this means that they summarise data with a set of parameters, after we use the training data it becomes "useless" to us.

k-Nearest neighbours is a **non-parametric model** - one that cannot be characterized by a bounded set of parameters: this means that for each prediction we make we will consider ALL training examples, including the one(s) from previous predictions - this set grows over time, thus non-parametric. This approach is also called **instance-** or **memory-based training**.

Imagine we have a graph of points, some are of one type, some of another, and we introduce a new parameter (we call this a hyperparameter), can we predict which type this new point belongs to?

![[chrome_JxJfOGNEao.jpg]]

*k* is the number of closest values that we consider - we might want to select an odd k, and make sure that k is not a factor of the number of classes to prevent draws.

![[chrome_acHctdYKQd.jpg]]
![[chrome_092zQUXgz1.jpg]]

We can think of the Manhattan distance on a 2d-plane as going in the direction of x, then the direction of y, combined. The hamming distance is used when we are talking about booleans, often 1 for true, and 0 for false. The Euclidean distance is the normal Pythagorean distance, the shortest straight line path between two points on a 2d plane.

If we have a stalemate between distances, eg more than k values are equal and the smallest distances, then we can either just pick the first k values, or extend our k to include all the values.

![[chrome_IK4SioaUrl.jpg]]
![[chrome_Ky3NdnRbxr.jpg]]
The red-highlighted point picture above has the shortest distance, and so the new point is classified in the same class at $x^{(3)}$.

**k-NN Algorithm**

INPUT: training examples $x^{(i)}$ $\in$ X and the corresponding class $y^{(i)}$, a new query example $x^{(q)}$, number of neighbours $k$
OUTPUT: prediction for the class of the new query example

For each training example $x^{(i)}$ $\in$ X:
	-Calculate the distance between the training example  $x^{(i)}$ and the new query  $x^{(q)}$ 
	-Keep the best $k$ distances in a data structure T

Return the majority vote (or plurality in case of non-binary classification) of the class $y^{(i)}$ for the first k entries of T

**k-NN for Regression Problems**
If we consider a two-dimensional problem - we can find the k-NN for a query point and predict a value for the new example based on the average or median of the k nearest neighbours. 
![[chrome_B34IZbVKSg.jpg]]
The average is: $y^{(4)}$ = 2.9.

**An Issue and Resolution**
The problem with different numeric independent variables is that they might have different scales, e.g. 
$x_1$ might be in the interval 0 - 1 and $x_2$ might be in the interval 1 - 10. In this case, $x_2$ will affect the distance far more, so we first must normalise. 
![[chrome_74WafC4kpB.jpg]]

![[chrome_8LVo6sgEUo.jpg]]

In the algorithm above, we simply change the "calculate distance" to "calculate normalised difference".

![[chrome_ULdTkA10WM.jpg]]

**Pros and Cons of k-NN**
Pros
- Training is simple and fast: just store training data
- Finds the class of the new example based on most similar examples present in the training data
Cons
- It uses large space in memory: we need to store all data
- Running the algorithm can be slow if we have many training examples and many dimensions

*Complexity*
- Given a set of N examples and a query example, we calculate the distance to the query example from each one and keep the best k
- The above runs in O(N) with a (sequential) table
- However, these methods are designed for large datasets. Therefore, better data structure are desirable
- A binary tree runs in O(log N), whereas a hash table runs in O(1)

![[Pasted image 20230329160653.png]]

**Evaluation Procedures***

A supervised learning method consists of 3 main elements:
-Model, namely the form of function we want to learn
-Cost function, namely, the misfit between a particular function from the model
-Training algorithm, namely, gradient descent minimisation of cost function

Running the training algorithm on training data learns the "best" values of the free parameters, yielding a predictor. We use evaluation procedures to determine how good our model is. To do so, we randomly split the available annotated data into a training set (to estimate all the free parameters) and a test set (to evaluate the trained predictor before deployment).

![[chrome_fa34lU3thU.jpg]]
Say we wanted to choose between these three models, if we have trained them each on the training data and evaluated the performance of the predictor using the test set, then we can't use this data to decide between them, so we split the training data once more, adding a partition for validation.

We have a few different methods for training procedures:

Method 1 - **Holdout Validation**
1. Randomly choose 30% of data to form a validation set
2. Keep the rest in the training set
3. Train your model on the training set
4. Estimate the test performance on the validation set
5. Choose model with lowest validation error
6. Re-train with chosen model on joined training and validation to obtain predictor
7. Estimate future performance on test set
8. Ready to deploy predictor

For step 4, we employ different procedures for classification vs regression, classification we use (# wrong / # predictions) and regression we use mean square error.

![[chrome_XzgtukzOdW.jpg]]

Method 2 - **k-Fold Cross-Validation**
1. Randomly split the training set into k disjoint sets of equal size (k=3 in this example)
2. Use the k-1 of those together for training
3. Use the remaining one for validation
4. Permute the k sets and repeat k times
5. Average the performance on the k validation sets
![[chrome_C0AMIbKCsE.jpg]]
![[chrome_EiJyzYJRQA.jpg]]

Method 3 - **Leave-One-Out Validation**

We leave out a single example for validation, and train on all the remaining annotated data
For a total of N examples, we repeat this process N times, each time leaving out a single example
Take the average of the validation errors as measured on the left-out examples
Same as N-fold cross-validation where N is the number of labelled examples
This is extremely computationally expensive, but it doesn't waste any training data.