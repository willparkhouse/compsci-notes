An **agent** is an entity that perceives the world, and acts in this environment. A **problem-solving agent** is one that uses atomic representations (each state of the world is perceived as indivisible), and requires a precise definition of the problem and its goal/solution.

A **search problem** is one such that we need to search for a solution, e.g. games can be viewed as a search problem. The **formulation of a search problem** is the process of formally defining the search problem by making assumptions about the environment:
	-**Observable**; the agent is able to know the current state
	-**Discrete**; there are only finitely many actions at any state
	-**Known**; the agent can determine which states are reached by which action
	-**Deterministic**; each action has exactly one outcome

Formally a search problem is defined by the following:
	-Initial state; the state that the agent starts in
	-Actions; the set describing the actions that can be executed in any state
	-Transition model; the states resulting from executing each action, a, from every state, s (a description of what each action does)
	-Goal test; determines if a state is a goal state
	-Path cost; a function that assigns a value to each path

The first three above define the **state space**.

We can represent our search problem as a tree of states, with branches off for each action. At any given state we have a set of actions that the agent can perform, these children we call the **frontier** (sometimes referred to as the **open list**).

An **uninformed search** is one such that the agent has no additional information about states beyond what's provided in the problem formulation. 

An **informed search**, such as A\*, is one such that the algorithm has some knowledge of the outside world - in the case of A\* this is the heuristic function.

We can measure the performance of our algorithm based on **completeness** (whether the algorithm is guaranteed to find a solution provided one exists), **optimality** (is the strategy optimal), **time complexity** and **space complexity**.

In AI we use an implicit representation of the graph via the initial state, actions and transition model, therefore the following three quantities are used to measure performance: 
	-**branching factor**, the maximum number of successors of each node, **b**
	-**depth** of the shallowest goal node, **d**
	-the **maximum length** of any path in the state space, **m**

**Uninformed Search Strategies**

**Breadth-first search** is one of the most common search strategies, we can use a queue (FIFO) for expansion:
	-The root is expanded first
	-Then, all the shallowest nodes are expanded, do not add children in the frontier if the node is already in the frontier or in the list of visited nodes (to avoid loopy paths)
	-Repeat **until** the **first goal node** is reached, then stop

For breadth-first search:
	Completeness: if the goal node is at some finite depth d, then the BFS algorithm is complete as it will find it
	Optimality: if the path cost is a non-decreasing function of the depth of the node, then BFS is optimal
	Space complexity: O(b$^d$)
	Time complexity: O(b$^d$)

**Depth-first search** is another common search strategy, we can use a stack (LIFO) for expansion:
	-The **root** node is **expanded** first
	-Then, the **first** (or one at random) **successor** of the root node is **expanded**
	-Then, the **deepest node** in the **current frontier** is **expanded**

For depth-first search:
	Completeness: DFS is **not complete** if the search space is **infinite** or if we **do not check** for **infinite loops**; it **is complete** if the search space is **finite**
	Optimality: DFS is **not optimal** as it can expand too deep and miss a shallower goal node on one of the other children.
	Time Complexity: $O(b^m)$, as it depends on the **maximum length** of the path in the search space (m, the max, can be much larger  than d, the depth of the shallowest goal node)
	Space Complexity: $O(b^m)$, as we store all the nodes from each path from the root node to the leaf node.

There is a **memory saving variant** of DFS, in which paths we have fully expanded, with no goal node found, are removed from memory, this reduces the space complexity to $O(bm)$.

**Depth-limited search** is another variant on DFS in which we introduce an invariant, $l$, which is the depth limit. We only allow the DFS algorithm to search up to $l$ deep, and if no goal node if found, we remove the path up to the next deepest node, and continue the search from there. This solution is non-optimal is $l < d$, as we will never reach a goal node. The time complexity for this is $O(b^l)$ and space complexity is $O(bl)$.

![](Images/Pasted%20image%2020230212192311.png)

**Informed Search Strategies**

When we have a heuristic that informs the algorithm about information in the world, we can use more efficient algorithms.

The first is actually suboptimal, but we will start there: **Greedy best-first search** works by expanding the root node, and simply expanding the node with the smallest heuristic.

A smarter alternative to this is the A\* algorithm, which evaluates nodes using the cost function: $f(n) = g(n) +h(n)$, where $g(n)$ is the cost to reach the node, and $h(n)$ is the heuristic from that node to the goal. We must maintain that the heuristic is ***consistent***, which means that the estimate to a goal is no greater than the ground-truth value to a goal. $h(n) \leq cost(n,n') + h(n')$ 

**A* search algorithm**: 
	-**Expand** the node in the frontier with the **smallest cost** $f(n) = g(n) +h(n)$
	-**Do not add** children to the frontier if the node is **in the list of visited nodes** (to avoid loopy paths)
	-If the state of a given **child** is **in the frontier**, then we check if the frontier node has a **larger**  $g(n)$, if so **place the child into the frontier** and remove the node with larger $g(n)$ from the frontier
	-Stop when a goal node is visited

For the A* search algorithm:
	**Completeness**: A* is complete if the heuristic is consistent.
	**Optimality**: A* is optimal if the heuristic is consistent.
	**Space complexity**: O(b$^d$), since we keep all expanded nodes and all node in the frontier, in memory.
	**Time complexity**: O(b$^{\epsilon d}$), where $\epsilon$ is the relative error of the heuristic calculated: $\epsilon =\frac{h^* - h}{h^*}$, where $h^*$ is the actual cost from root to goal node and $h$ is the heuristics value.

![](Images/Pasted%20image%2020230212193004.png)