A B-Tree, of order m, is an n-ary tree with the following properties:
	-Every node has **at most m children**
	-Every **non-leaf node**, except the root, has at least **m/2 children**.
	-The **root** node, if it is not a leaf node, has **at least 2 children**.
	-A non-leaf node with c children, contains c-1 search key values, which act as separators or discriminators, to guide searches down the appropriate sub-trees.
	-All **leaf** nodes appear in the **same level**
	-The tree is **always height balanced**
	-Update and search is O(log N)

Basic B-Tree's, as outlined above, are not often used in practice, but instead a variant, the B+Tree, is more frequently used.

**External Data Structures**
Data structures such as linked lists, queues, BSTs, AVL trees, are often stored in memory, as they are useful for computing on the go, but some data structures are useful for storing data in disks (aka secondary memory). The differences of storing to secondary memory play a large factor in deciding what data structure to hold this data in. These differences are:
	-The data structure can persist beyond the end of the program execution, without the programmer having to explicitly save or load the structure to/from disk.
	-It can grow to sizes larger than can fit in internal memory.
	-These data structures can be accessed and updated by multiple different programs simultaneously. 

Some properties of secondary storage to consider:
	-Disks store data in blocks, of size configured by the operating system, usually 4KB, but can be up to 64KB.
	-The disk can only transfer whole blocks at a time: to write a single byte to disk would have to copy the block into memory, replace the byte, and write back to the disk.
	-In spinning hard disks, the time to read (or write) a block includes:
		1. time to move the disk head to the correct track (6ms)
		2. time to wait for the block to spin around to the disk head (4ms)
		3. time for the disk to spin further until the entire block has passed under the disk head
		4. reading or writing consecutive blocks is much faster than reading a random sequence of disk blocks.

**B+Trees**
Terms:
	**Data record**: an element of data information to be managed by the B+Tree
	**Key Value**: the value by which we identify the record, e.g. an ID or index.
	**Discriminator**: value used to decide which path to take down the tree in searching for a record.
	**Disk Address**: the offset from the start of the disk file to a particular block in the file. A file read/write can be executed by requesting the operating system to 'seek' to this address and read/write a block from/to the file. Analogous to a point in memory.

B+Trees nodes are designed to fit in disk blocks so that reading or writing a node corresponds to reading or writing a single block. We try to **fit as many** sub-tree pointers into **each node as possible** while maintaining this constraint of fitting in **one disk block**. 

In real-world B+Tree implementations, the **key values** are often **variable** length strings, and the limiting factor on the number of children is not some arbitrary order specification, but dictated by the amount that can fit within a single block.

There are 2 common variants of B+Trees:
	**Record-Embedded B+Trees**: this variant stores the data records in the leaf nodes. This is a suitable data-structure when you only need to find records by one search key. e.g. searching by ID but not by name.
	**Index B+Trees**: this variant stores only key values and disk addresses in the leaf nodes. The data records are stored in separate files. This means that the size of the tree is smaller than a Record-Embedded tree, and also multiple B+Tree indexes can be created on a collection of data records

**Record-Embedded B+Trees**

![[chrome_pKcFT6ivkA.jpg]]

**Operations**
Search: start at the root and follow the path down indicated by the discriminator values, go to the node identified by the disk address whose left discriminator is less than the search key and whose right discriminator is greater than OR equal to the search key.

Range: Search for the record with a key at one end of the range, then iterate, using the next/prev disk addresses in the leaf level, over all records in the range. So if we were searching for the range `[15, 20]` we could 

Insert: Search with the key of the record to be inserted to find the location to insert the record. If there is space, insert the record. If there is not enough space, split the block in two so that approximately half the number of records go to the left and half to the right. Then you **post** (insert to the level above) the key value of the lowest record in the right page as a discriminator. If there is no room for the parent node to hold this new discriminator, you continue splitting and posting until either the insertion is complete or the root node of the tree is split.

Insert Properties: 
	-The tree only grows in height when the root node splits, every path from the root to any leaf is the same.
	-No node is ever less than (approx) half full, except for the root node.
	-Deletion works as an inverse of insertion. If you delete an element from a node and it becomes less than half full, we check to two adjacent nodes. If these are larger than half full, we "steal" or "distribute" some of the adjacent nodes elements within the less-than half full node. If either of the adjacent nodes are half-full then we can merge the two nodes. This may cause a cascade upwards and force internal parent nodes to also merge.

**Bulk Loading**
When creating a B+Tree you could insert as described above, but this is quite inefficient when you have a large dataset as you have to perform a large amount searches. We can instead perform a **Bulk Load**, this involves:
	-Sorting the records and inserting them directly into leaf nodes, connecting the nodes as you go, like a doubly linked-list.
	-As you construct the leaf nodes, construct a parent node and add pointers toward each of the leaf nodes, and when a parent node is full you can split this parent node (by any percentage, say 50/50 or 70/30) creating new parent nodes as needed.
The result in all the splits happening along the rightmost path in the tree, rather than randomly distributed across many different paths, which in turn means that, even with very large trees, one can hold the rightmost path in memory and only write blocks when they become full, resulting in far greater performance.

**Index B+Tree**
There are two sub-variants of index B+Tree:
	-**Secondary Index B+Tree**: Here the records are **NOT necessarily
	sorted** in the data file of blocks. Thus each entry in the leaf nodes of  
	the tree contains a pair consisting of a key value and a disk address  
	which identifies the disk block where the record with that key value  
	can be found. Leaf nodes also have forward and reverse pointers.  
	-**Primary Index B+Tree**: Here the records are kept **sorted** by the  
	key value used in the B+Tree in the data file containing the blocks  
	of records. Thus the leaf nodes of the B+tree only need to store  
	discriminator values to separate the data file blocks and look much  
	like internal B+Tree nodes except that they also have forward and  
	reverse pointers.

![[chrome_EYMXNrroJV.png]]
Primary index B+Trees are useful if the data stored in the record file is large, and you don't want to store it in the tree itself, but externally to save space in the tree and make use of the efficient search a B+Tree offers.

![[chrome_iehJu5Qhwd.png]]
Secondary Index B+ Trees are useful if you have multiple different indexes for a given set of data. The data may be sorted by a different attribute, but you want to also be able to easily index them via another attribute.

**Complexity of B+Tree**
B+Tree is an n-ary tree which is height balanced, so the search is O(log N).
In practice, search will take $log_{m}n$ reads of pages, where the fanout ratio of the tree is m (fanout ratio is the number of children in each node). Insertion has the same time complexity.

With a 4KByte block, a 4 byte integer key, a 4 byte disk address size, with every block being half full, and even assuming a little space is n each block is taken with extra administrative information, that gives a fanout ratio of at least 250.  
•Tree of height 1: 250 records  
•Tree of height 2: 62,500 records  
•Tree of height 3: 15,625,000 records  
•Tree of height 4: 3,906,250,000 records

This means an element in a tree of height 4 with 4 billion entries can be located in 4 disk reads. Although in practice, we would normally cache all but the bottom two levels in memory, so it would only take 2 disk reads.

