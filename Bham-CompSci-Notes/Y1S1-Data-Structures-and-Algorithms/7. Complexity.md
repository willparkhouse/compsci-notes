To compare two algorithms we need a method that is irrespective of the hardware it's running on, the optimization of the compiler, and various other context specific factors. There are two dimensions of performance we are usually interested in, **space** and **time**.

We measure **time** in number of steps: e.g. # of additions, multiplications, comparison operation, memory accesses.

We measure the **space** in memory, as one algorithm might take 5% faster than another but require 5x more space in memory, which might not be preferential.

We need to compare the space and time performance scales with the size of the problem, so we parametrize the performance by the size of the input.

We can calculate either the **average case time complexity**, or the **worst-case time complexity**. See below, a comparison of different growths of algorithms as `n` grows.

![[chrome_O6yy6uZAW3.png]]

In BigO notation, we discount all variables except the one of largest power. E.G. n^2 + 5n simplifies to n^2 as the 5n part has little effect on the larger complexity.

There are various different types of measuring / comparing time-complexity. 'Big O' is the most commonly used, but also 'Little o', 'Theta', 'Asymptotically Equal' and 'Omega' are sometimes used. 

**Big O**
The formal definition of Big O notation is as below:
**f (n) = O(g(n))** is equivalent to **|f (n)| ≤ |Cg(n)|**, which basically means that O means you can find some constant for which the g(n) is larger than f(n), f(n) grows at the same rate, or slower, than g(n). 

E.G. `2n^2 + n = O(n^2)` because we can find some constant (eg 1000) which makes n^2 larger than f(n). So we can have f(n) > g(n), for all n, but NOT f(n) > Cg(n). Big O only refers to relative growth rate, NOT relative speed X or memory usage.

**Little o**
**f (n) = o(g(n)) ⇐⇒ lim as n→∞ of ( f (n) / g(n) ) = 0**
What this means is that we choose a function for little o that is ALWAYS larger than f(n). In simpler terms, think about the time complexity of n^2 + 200n + 20 in BigO and littleo. This is O(n^2), which is fine as a ballpark, but this isn't very precise as the coefficient for n^2 is 2. Below see the evaluation for little o:
![[chrome_yAX3GYEFVt.jpg]]
We take the limit as n tends to infinity, and the littleo is only valid if f(n)/g(n) tends to 0.

**Theta**
**f (n) = Θ(g(n)) ⇐⇒ c1g(n) ≤ f (n) ≤ c2g(n)**  (where c1 and c2 are positive co-effs)
This restricts the upper bound x of the complexity estimation, f and g must have the same rates of growth, in other words f is bounded above and below by some multiples of g.
![[chrome_xFgJQ70pZ2.jpg]]

**Asymptotically Equal**
**f (n) ∼ g(n) ⇐⇒ lim as n→∞ of (f (n) / g(n)) exists and is equal to 1**
Asymptotically Equal is useful to very closely model the growth rate of the algorithm.
![[chrome_6wgVAMnOhL.jpg]]

**Omega**
**f (n) = Ω(g(n)) ⇐⇒ |f (n)| ≥ |cg(n)|**
This provides a lower bound on f: as f grows, it will always grow at least at the same rate as g, and it could grow faster.

**Different kinds of complexities**
	-**Average** Case complexity = average complexity over all possible inputs/situations (we 
		need to know the likelihood of each of the input!) 
	-**Worst** Case complexity = the worst complexity over all possible inputs/situations 
	-**Best** Case complexity = the best complexity over all possible inputs/situations
	-**Amortized** complexity = average time taken over a sequence of consecutive operations

**Amortized Complexity**  
**Amortized complexity** is the total expense per operation, evaluated over a sequence of operations. We work this out by calculating the total cost over the sequence, and dividing by the total n.

Say we have an unsorted array, we can do a linear search to find an element. We could instead sort the array, THEN use a binary search (which is much faster on sorted arrays). This isn't computationally optimal with 1 search, but might be with 10 or 50 searches. Amortized complexity helps us figure out where the boundary is. We would work out the time complexity of all the individual linear searches combined, and separately work out the time complexity of first sorting the array, then doing a series of binary searches.

