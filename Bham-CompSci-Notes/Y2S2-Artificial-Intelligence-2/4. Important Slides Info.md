#### Bayes' Theorem for Probability Distributions
Given two random variables, $X$ and $\Theta$:
$p(\Theta | x ) =\frac{p(x |\Theta )p(\Theta )}{p(x)}$
where $p(\Theta | x )$ is called the **posterior distribution**, $p(x |\Theta)$ is the **likelihood function**, $p(\Theta)$ is called the **prior**, $x$ is a value of $X$, and $p(x)$ is a scalar called **marginal likelihood**. In plain English:
$posterior \:distribution = \frac{prior\: distribution × likelihood\: function} {marginal\: likelihood\: (model\: evidence)}$

1. **Posterior Distribution** $p(\Theta | x )$: This is what you're aiming to find out. The posterior distribution represents the probability of the parameter $\Theta$ given the observed data $x$. After observing the data, this distribution updates your belief about the uncertainty of $\Theta$.
2. **Likelihood Function** $p(x |\Theta)$: This is a function of $\Theta$ given fixed data $x$. It measures how probable the observed data $x$ is, for different values of $\Theta$. The likelihood is not a probability distribution over $\Theta$ because it does not integrate to 1 over $\Theta$. Instead, it's used to weigh the plausibility of different values of $\Theta$ based on the observed data.
3. **Prior Distribution** $p(\Theta)$: This represents what is known about $\Theta$ before observing the data $x$. The prior distribution reflects your initial beliefs about the parameter's values. It can be based on previous studies, expert knowledge, or even be a subjective choice if there's no prior information.
4. **Marginal Likelihood** $p(x)$: This is also known as the evidence. It is a normalization factor that ensures the posterior distribution is a proper probability distribution (i.e., it integrates to 1 over all possible values of $\Theta$).

There is a connection between the **likelihood function** and the **posterior distribution** in Bayesian inference, but there's a nuanced distinction in their roles and what they represent. While both the likelihood function and the posterior distribution relate to how the observed data $x$ informs us about the parameter $\Theta$, they do so in different ways:
- **Likelihood:** Measures how well different parameter values explain the observed data, without considering prior beliefs about those parameter values.
- **Posterior Distribution:** Represents the updated beliefs about the parameter values, integrating both the likelihood of the observed data given those values and the prior beliefs about the parameter values.

#### Example
$\theta$ represents a specific disease, $x$ represents a specific symptom. The joint PMF is:
![](Images/chrome_SHLT9xYpWa.png)

What are the conditional probabilities that a patient each of these diseases given that they have the symptom $x_3$? 

$p(\Theta | x_3 ) =\frac{p(x_3 |\Theta )p(\Theta )}{p(x_3)} =\{0, 1/71, 0.01, 0.14, 0.22, 0.15, 0.17, 0.01, 0.11, 0.07\}$

In the Bayesian interpretation of the likelihood function, $\Theta$ is a random variable. A function whose value is proportional to the probability of the conditional probability distribution $X$ given $\Theta$.

What is the most possible disease given the patient has the symptom $x_3$?

We can trivially see that this is $\theta_5$, but for larger distributions we have a tool for this called the **Maximum a posteriori probability** (MAP) estimate. 

$\hat{\theta}_{MAP} = \underset{θ}{argmax} \; p(\Theta|x_3)=\theta_5$  

This is very similar to the maximum likelihood estimation (MLE), but the main difference between MLE and MAP is the incorporation of the prior distribution $p(\Theta)$  in MAP. MLE can be seen as a special case of MAP where the prior $p(\Theta)$ is uniform or non-informative. When the prior is uniform, the MAP estimate becomes equivalent to the MLE estimate because the prior does not influence the maximization—only the likelihood function does.
### Bayesian Networks
A Bayesian network is a probabilistic graphic model, in which nodes represent random variables, and edges represent conditional independence assumptions. All edges in Bayesian networks are directed. The edges are used to represent cause-effect relationships.

![](Images/chrome_UVFwpnPAr3.png)
*A famous example of a Bayesian network*

There are many standard structures in Bayesian networks. In the below photo A is a direct *cause* of B, we could also say that B is a direct *effect* of A.
![](Images/chrome_t88OAUy42s.png)
Each node in a Bayesian network has a conditional distribution given its parents:$P(X_i|Parents(X_i))$ 
This conditional distribution can be represented as a condition probability table (CPT) - the distribution over $X_i$ for each combination of parent values.

![](Images/Pasted%20image%2020240412192205.png)

A normal **full joint distribution** is calculated:
$P(X_1,X_2,...,X_n) = \prod_{i=1}^n \: P(X_i|X_{i-1},...,X_1)$   

but in the context of Bayesian networks, we define the full joint distribution based of the edges as the product of the local conditional distributions:
$P(X_1,X_2,...,X_n) = \prod_{i=1}^n \: P(X_i|Parents(X_i))$   

This is the essence of a Bayesian network - a **Bayesian network** is a **compaction representation of a join probability distribution in terms of conditional distribution**.

Normally, representing the joint distribution of a large number of random variables explicitly can be prohibitively complex. For $n$ random variables, each with $k$ possible states, the full joint distribution would typically require $k^n−1$ probabilities to be specified (since the total probability must sum to 1).

However, Bayesian networks drastically reduce this complexity by exploiting conditional independence properties between variables. Instead of representing all possible combinations of variable states directly, a Bayesian network represents these dependencies using a directed acyclic graph (DAG).
#### Conditional Independence 
**Conditional independence** is a fundamental concept in probability theory that implies that the probabilistic relationship between two or more variables can be decoupled given the knowledge of the state of certain other variables. In the context of Bayesian networks, this property allows us to simplify the representation of joint probability distributions significantly.

Two sets of variables A and B are **independent** iff $P(A) = P(A|B)$ or equivalently $P(A,B)=P(A)P(B)$.

Two random variables A and B are conditionally independent if they are independent given a third random variable C, written as: $\newcommand{\indep}{\perp \! \! \! \perp} (A\indep B) | C \iff P(A,B |C) = P(A|C)P(B|C)$
![](Images/chrome_4A5sUYNJe1.png)
*Graphical Representation of Conditional Independence*

Suppose A, B and C can take one of 2 values - the complete join distribution would require $2^3 -1 = 7$ parameters, in contrast using conditional independence we need $1 +2 +2=5$ parameters.
#### Probabilistic Relationship: Indirect Cause
![](Images/chrome_5D3nlBiYZ1.png)
Here since *WetGrass* is independent of *Cloudy* given *Rain*, we have the joint probability distribution:
$P(C,R,W)=P(C)P(R|C)P(W|R)$ 
#### Probabilistic Relationship: Common effect
![](Images/chrome_YYON9d6E9p.png)
Suppose we identify another cause of wet grass, the sprinkler, we can then model the relationship as a converging connection. The joint probability distribution is :
$P(S, R, W ) = P(S)P(R)P(W |S, R)$

If neither W nor any of its descendants are observed, S and R are independent. If we can confirm one cause (e.g., rain is true) of an observation (e.g., grass is wet), it reduces the need to invoke alternative causes, e.g., sprinkler is on.
#### Probabilistic Relationship: Common Cause
It is natural to suppose that cloudy weather influence our decision to turn on the sprinkler or not and cloudy weather will also influence the chance of rain, we model this “common cause” probabilistic relationship using:
![](Images/chrome_rLQXbgpk4y.png)
The joint probability distribution:
$P(C, S, R) = P(C)P(S|C)P(R|C)$ 

If C is observed, then S and R are independent.