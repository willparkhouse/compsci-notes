#### Self Information
Self-information quantifies the information content of an event. Shannon proposed self-information based on three axioms:
- An event with a 100% probability is not surprising and yields no information.
- The less probable an event, the more surprising and more informative it is.
- If two independent events are measured separately, the total amount of information is the sum of the self-information of the individual events.

Self-information of outcome 'X' when measuring a random variable 'X' with probability mass function (PMF), 'PX(x)', is defined as --> $IX(x)=-logb[PX(x)]$

Here, 'b' is the bases of the logarithm, and different bases result in different units: b=2 (bits), b=e (natural units or nat), b=10 (dits, bans, or hartleys).
#### Logit and Self-Information 
Given some event 'x', with 'p(x)' as the probability of 'x' occurring, and $p(¬x) = 1 − p(x)$ as the probability of 'x' not occurring. Logit is defined as:
$logit(x) = log (p / 1 − p) = log(p) - log(1 - p)$. 
It can be connected with self-information as:
$logit(x)= I (¬x) - I (x)$
#### Entropy
Entropy measures "the uncertainty in a random variable X". For a **discrete** random variable 'X' with range 'RX={x1,...,xn}', and its PMF as 'PX(x)', entropy is defined as:

$H(X)= E[I_X(x)] = - ∑_i^n P(X = x_i)*logb P(x = x_i) = - E[logb PX(x)]$

Breakdown of the variables in the above formula:
1. **$H(X)$**: This denotes the entropy of the random variable 'X'. It measures the uncertainty or the amount of information in the variable 'X'.
2. **$X$**: This represents a discrete random variable. A random variable can be thought of as a variable whose values depend on outcomes of a random phenomenon.
3. **$x_i$**: These are the specific outcomes that the random variable 'X' can take. The range $RX={x1,...,xn}$ means 'X' can take any value from 'x1' to 'xn'.
4. **$P(x)$** or **$P(X = x_i)$**: This denotes the probability mass function (PMF) at a particular outcome 'x_i'. In simple terms, it refers to the probability of the variable 'X' taking the specific value 'x_i'.
5. **$E$**: The expectation or average value of a random variable is represented by 'E'. It is essentially a weighted average where each possible outcome is weighted by the probability of that outcome.
6. **$I_X(x)$**: This represents the self-information of the outcome 'x' when measuring 'X'. It gauges the amount of information or surprise derived when the outcome 'x' is observed.

In plain English, the entropy 'H(X)' is the expected surprise or expected self-information when measuring 'X'. This formula essentially sums up the individual self-information of each possible outcome weighted by the probability of that outcome.
#### Joint entropy
Joint entropy is a concept in information theory that quantifies the uncertainty associated with a set of variables, specifically discrete random variables X and Y. It provides a **measure of** how much **uncertainty** exists **when considering both variables simultaneously**.

The joint entropy, denoted as H(X, Y), is defined using the probability distribution of the joint occurrences of values of X and Y. Mathematically, it is represented as:

$H(X, Y) = -E[log p(X, Y)] = \sum_{x_i\in R_X} \sum_{y_j\in R_Y} p(x_i,y_j)log(p(x_i,y_j))$
Where:
- $E[]$ denotes the expected value, or the average over all possible values of X and Y.
- $p(X, Y)$ represents the joint probability mass function (PMF), which gives the probability of both X and Y taking specific values simultaneously.
- The summation is performed over all possible values $x_i$ of X and $y_j$ of Y.

In simpler terms, joint entropy calculates the average uncertainty associated with the joint occurrences of values of X and Y. It is derived from the negative of the expected value of the logarithm of the joint probability distribution. The higher the joint entropy, the greater the uncertainty or randomness associated with the combined values of X and Y.

#### Chain rule for conditional entropy
$H(Y|X) = H(X, Y) - H(X)$

![](../../../chrome_x9A2RoEM4p.png)

#### Mutual Information
Mutual information quantifies the degree of dependence or **shared information** between two random variables, often denoted as $X$ and $Y$. 

Intuitively, mutual information captures how much knowing one variable reduces the uncertainty about the other variable. If knowing the value of $X$ provides insights or reduces uncertainty about the possible values of $Y$, then they are likely to have high mutual information and vice versa.

Mathematically, the mutual information between X and Y, denoted as $I(X;Y)$, is defined as:

$I(X;Y) = ∑_{x∈R_X} ∑_{y∈R_Y} p(x, y) log \frac {p(x, y)} {p(x) p(y)}$

We can also express mutual information in terms of joint and conditional entropies: 

$I(X; Y ) ≡ H(X) − H(X|Y )$
$≡ H(Y ) − H(Y |X)$
$≡ H(X) + H(Y ) − H(Y , X)$
$≡ H(X, Y ) − H(Y |X) − H(X|Y )$

![](../../../chrome_0XMyWlbFxy.png)
*A visualisation of the above equations.*

#### Properties of Mutual Information
- Non-negativity: $I(X; Y ) ≥ 0$ (mutual information between $X$ and $Y$ is always greater than or equal to zero)

- Symmetric: $I(X; Y ) = I(Y ; X)$ (mutual information between $X$ and $Y$ is symmetric, meaning it doesn't matter which variable we consider as the "source" or "target" of information)

- Measure of dependence between X and Y : 
	$I(X; Y ) = 0$ iff $X ⊥ Y$, i.e. $X$ and $Y$ are independent.
	$I(X; Y )$ not only increases with the dependence of $X$ and $Y$ but also with $H(X)$ and $H(Y )$ 

- $I(X; X) = H(X) − H(X|X) = 0$ (The mutual information between $X$ and itself is equal to the entropy of $X$ minus the conditional entropy of $X$ given itself, which is always zero. This indicates that $X$ carries no additional information about itself beyond its inherent uncertainty, resulting in zero mutual information.)
#### Decision Tree Learning and Entropy
Decision trees are popular machine learning algorithms known for their **simplicity**, **interpretability**, and **extendibility**.

Decision Tree Components:
- **Predictive Model**: Tree structure with root/internal nodes representing independent variables (features) and leaves representing outcomes of the dependent variable.
- **Types**: Classification trees (categorical dependent variables) and regression trees (continuous dependent variables).

Some popular algorithms for decision trees are: ID3, C4.5 and CART.

How should we construct a decision tree? Can we learn the structure from the data?
#### Information Gain
**Information gain** is the information we can gain after spiting the samples based on a independent variable (internal node). 

Formally, information gain is defined as the change in information entropy $H$ from a prior state to a state that takes some information as given: 

$IG(Y , X) = H(Y ) − H(Y |X)$

where $Y$ is a random variable that represent the dependent variable (labels) and $X$ is one of the independent variable, and $H(Y |X)$ is the conditional entropy of $Y$ given $X$. 
#### Mutual Information Feature Selection
Notice that **information gain** *is* actually **mutual information**. What this means is that decision tree learning algorithms (ID3 and C4.5) recursively use mutual information to select the independent variable that share the most information with the dependent variable, then split (make decision) the samples based on the value of this independent variable.

Given that information gain *is* mutual information, we could select a set of most relevant independent variables based on mutual information and use other powerful machine learning algorithms to build predictive models. This is called **feature selection**. We use mutual information to choose a optimal set of independent variables, called features that allow us to classify samples. 

Formally, given an initial set $F$ with n independent variables, $X = \{X_1, X_2 . . . , X_n\}$, find subset with $S ⊂ F$ features that maximises the Mutual Information $I(Y ; S)$ between the dependent variable Y (label), and the subset of selected features S.

![](../../../chrome_elXEijKtEU.png)