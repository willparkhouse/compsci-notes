#### Information Theory Essentials
Information Theory ties practical solutions to address communication challenges initially proposed by Shannon. Its importance lies in widespread applications ranging from source coding, channel coding (used in error correction), audio/image/video compression to establishing connections with probability & statistics. It bridges the gap between AI and physics.
#### Why Information Theory Exists in AI 
Information Theory and Machine Learning are two sides of the same coin. It aids in:
- **Feature Selection**: This process involves selecting meaningful attributes to improve model performance. Information Theory provides a foundation through Conditional Likelihood Maximisation in Information Theoretic Feature Selection.
	- Clinical Decision Making: Doctors can opt for the most informative test, thus leading to effective diagnosis and treatment. 
- **Unsupervised Learning**: Clustering, a type of unsupervised learning, can use a mutual information criterion.
- **Supervised Learning**: Information Bottleneck method, rooted in Information Theory, offers a theoretical framework for the deep learning domain. Cross-entropy, a concept from Information Theory, is used as the loss function for Neural Networks and Logistic Regression.
	- Decision Tree Learning: Information Theory contributes a useful criterion to select a splitting property.
	- Bayesian Learning: It offers a basis for choosing the best model given some data.
#### An Example
Consider these two propositions: **Shan is a human**. **Shan is a man**. How can we quantify the difference between two sentences?  

Shannon proposed that the “semantic aspects of data are irrelevant”, i.e., the nature and meaning of data are not the information per se. Instead he quantified information in terms of probability distribution and “uncertainty” (surprise).
#### Self Information
Self-information quantifies the information content of an event. Shannon proposed self-information based on three axioms:
- An event with a 100% probability is not surprising and yields no information.
- The less probable an event, the more surprising and more informative it is.
- If two independent events are measured separately, the total amount of information is the sum of the self-information of the individual events.

Self-information of outcome 'X' when measuring a random variable 'X' with probability mass function (PMF), 'PX(x)', is defined as --> $IX(x)=-logb[PX(x)]$

Here, 'b' is the bases of the logarithm, and different bases result in different units: b=2 (bits), b=e (natural units or nat), b=10 (dits, bans, or hartleys).
#### Logit and Self-Information 
Given some event 'x', with 'p(x)' as the probability of 'x' occurring, and $p(¬x) = 1 − p(x)$ as the probability of 'x' not occurring. Logit is defined as:
$logit(x) = log (p / 1 − p) = log(p) - log(1 - p)$. 
It can be connected with self-information as:
$logit(x)= I (¬x) - I (x)$
#### Entropy
Entropy measures "the uncertainty in a random variable X". For a **discrete** random variable 'X' with range 'RX={x1,...,xn}', and its PMF as 'PX(x)', entropy is defined as:

$H(X)= E[I_X(x)] = - ∑ PX(X = x_i)*logb P(x = x_i) = - E[logb PX(x)]$

Breakdown of the variables in the above formula:
1. **$H(X)$**: This denotes the entropy of the random variable 'X'. It measures the uncertainty or the amount of information in the variable 'X'.
2. **$X$**: This represents a discrete random variable. A random variable can be thought of as a variable whose values depend on outcomes of a random phenomenon.
3. **$x_i$**: These are the specific outcomes that the random variable 'X' can take. The range $RX={x1,...,xn}$ means 'X' can take any value from 'x1' to 'xn'.
4. **$PX(x)$** or **$P(X = x_i)$**: This denotes the probability mass function (PMF) at a particular outcome 'x_i'. In simple terms, it refers to the probability of the variable 'X' taking the specific value 'x_i'.
5. **$E$**: The expectation or average value of a random variable is represented by 'E'. It is essentially a weighted average where each possible outcome is weighted by the probability of that outcome.
6. **$I_X(x)$**: This represents the self-information of the outcome 'x' when measuring 'X'. It gauges the amount of information or surprise derived when the outcome 'x' is observed.

In plain English, the entropy 'H(X)' is the expected surprise or expected self-information when measuring 'X'. This formula essentially sums up the individual self-information of each possible outcome weighted by the probability of that outcome.
#### Examples
**Fair and Biased Coins**: 
- A fair coin 'X' has entropy:
	$H(X) = - ∑ P(xi)*log2 P(xi) = - ∑ (1/2)*log2(1/2) = - ∑ (1/2)*(-1) = 1$

- A biased coin 'Y' with heads probability of 0.7 has entropy
	$H(Y) = - ∑ P(xi)*log2 P(xi) = -0.7*log2(0.7) - 0.3*log2(0.3) ≈ 0.8816$

**Quantifying Information of English**: To quantify the information of English, treat it as a discrete random variable 'X' with range 'X= {1,....27}, where values 1-26 represent 26 letters and 27 represents a space. The PMF of 'X' is obtained by calculating the experimental probabilities of the 27 characters from a book. The entropy is then calculated as:
$H(X) = - ∑ P(x_i)*log2 P(x_i) = 4.11 (bits/letter)$.

![](Images/chrome_kHSoW8aMrg.png)