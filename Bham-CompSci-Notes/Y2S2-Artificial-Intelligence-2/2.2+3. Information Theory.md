#### Information Theory Essentials
Information Theory ties practical solutions to address communication challenges initially proposed by Shannon. Its importance lies in widespread applications ranging from source coding, channel coding (used in error correction), audio/image/video compression to establishing connections with probability & statistics. It bridges the gap between AI and physics.
#### Why Information Theory Exists in AI 
Information Theory and Machine Learning are two sides of the same coin. It aids in:
- **Feature Selection**: This process involves selecting meaningful attributes to improve model performance. Information Theory provides a foundation through Conditional Likelihood Maximisation in Information Theoretic Feature Selection.
	- Clinical Decision Making: Doctors can opt for the most informative test, thus leading to effective diagnosis and treatment. 
- **Unsupervised Learning**: Clustering, a type of unsupervised learning, can use a mutual information criterion.
- **Supervised Learning**: Information Bottleneck method, rooted in Information Theory, offers a theoretical framework for the deep learning domain. Cross-entropy, a concept from Information Theory, is used as the loss function for Neural Networks and Logistic Regression.
	- Decision Tree Learning: Information Theory contributes a useful criterion to select a splitting property.
	- Bayesian Learning: It offers a basis for choosing the best model given some data.
#### An Example
Consider these two propositions: **Shan is a human**. **Shan is a man**. How can we quantify the difference between two sentences?  

Shannon proposed that the “semantic aspects of data are irrelevant”, i.e., the nature and meaning of data are not the information per se. Instead he quantified information in terms of probability distribution and “uncertainty” (surprise).
#### Self Information
Self-information quantifies the information content of an event. Shannon proposed self-information based on three axioms:
- An event with a 100% probability is not surprising and yields no information.
- The less probable an event, the more surprising and more informative it is.
- If two independent events are measured separately, the total amount of information is the sum of the self-information of the individual events.

Self-information of outcome 'X' when measuring a random variable 'X' with probability mass function (PMF), 'PX(x)', is defined as --> $IX(x)=-logb[PX(x)]$

Here, 'b' is the bases of the logarithm, and different bases result in different units: b=2 (bits), b=e (natural units or nat), b=10 (dits, bans, or hartleys).
#### Logit and Self-Information 
Given some event 'x', with 'p(x)' as the probability of 'x' occurring, and $p(¬x) = 1 − p(x)$ as the probability of 'x' not occurring. Logit is defined as:
$logit(x) = log (p / 1 − p) = log(p) - log(1 - p)$. 
It can be connected with self-information as:
$logit(x)= I (¬x) - I (x)$
#### Entropy
Entropy measures "**the uncertainty in a random variable X**". For a **discrete** random variable 'X' with range 'RX={x1,...,xn}', and its PMF as 'PX(x)', entropy is defined as:

$H(X)= E[I_X(x)] = - ∑_i^n P(X = x_i)*logb P(x = x_i) = - E[logb PX(x)]$

Breakdown of the variables in the above formula:
1. **$H(X)$**: This denotes the entropy of the random variable 'X'. It measures the uncertainty or the amount of information in the variable 'X'.
2. **$X$**: This represents a discrete random variable. A random variable can be thought of as a variable whose values depend on outcomes of a random phenomenon.
3. **$x_i$**: These are the specific outcomes that the random variable 'X' can take. The range $RX={x1,...,xn}$ means 'X' can take any value from 'x1' to 'xn'.
4. **$PX(x)$** or **$P(X = x_i)$**: This denotes the probability mass function (PMF) at a particular outcome 'x_i'. In simple terms, it refers to the probability of the variable 'X' taking the specific value 'x_i'.
5. **$E$**: The expectation or average value of a random variable is represented by 'E'. It is essentially a weighted average where each possible outcome is weighted by the probability of that outcome.
6. **$I_X(x)$**: This represents the self-information of the outcome 'x' when measuring 'X'. It gauges the amount of information or surprise derived when the outcome 'x' is observed.

In plain English, the entropy 'H(X)' is the expected surprise or expected self-information when measuring 'X'. This formula essentially sums up the individual self-information of each possible outcome weighted by the probability of that outcome.
#### Examples
**Fair and Biased Coins**: 
- A fair coin 'X' has entropy:
	$H(X) = - ∑_{i+1}^n P(xi)*log2 P(xi) = - ∑_{i+1}^n (1/2)*log2(1/2) = - ∑_{i+1}^n (1/2)*(-1) = 1$

- A biased coin 'Y' with heads probability of 0.7 has entropy
	$H(Y) = - ∑_{i+1}^n P(xi)*log2 P(xi) = -0.7*log2(0.7) - 0.3*log2(0.3) ≈ 0.8816$

**Quantifying Information of English**: To quantify the information of English, treat it as a discrete random variable 'X' with range 'X= {1,....27}, where values 1-26 represent 26 letters and 27 represents a space. The PMF of 'X' is obtained by calculating the experimental probabilities of the 27 characters from a book. The entropy is then calculated as:
$H(X) = - ∑ P(x_i)*log2 P(x_i) = 4.11 (bits/letter)$.

![](Images/chrome_kHSoW8aMrg.png)
#### Joint PMF and Conditional PMF

1. **Joint Probability Mass Function (PMF):**
    - Imagine you have two random variables, let's call them X and Y.
    - The joint PMF tells you the probability that both X and Y take on specific values at the same time.
    - For example, if X represents the outcome of rolling a fair six-sided die (values 1 to 6) and Y represents the outcome of flipping a fair coin (either heads or tails), the joint PMF would tell you the probability of getting, say, a 3 on the die and heads on the coin flip at the same time.
    - Mathematically, it's represented as P(X = x, Y = y), where x and y are specific values of X and Y.
2. **Conditional Probability Mass Function (PMF):**
    - Conditional probability means we're interested in the probability of an event given that another event has already occurred.
    - So, the conditional PMF tells you the probability that one random variable (say, X) takes on a specific value given that the other random variable (say, Y) has taken on a specific value.
    - For example, if you know that the coin flip resulted in heads (Y = heads), the conditional PMF of the die outcome (X) would tell you the probability of rolling a 3, a 4, a 5, or a 6, since those are the outcomes where the die roll and the coin flip both lead to favourable events.
    - Mathematically, it's represented as P(X = x | Y = y), which reads as "the probability of X being x given that Y is y."
3. **Marginal Probability Mass Function (PMF):**
	- The marginal PMF focuses on the probabilities of individual events of a single random variable, ignoring the other variables.
	- It essentially summarizes the probabilities of one variable without considering the other variables in a multi-variable scenario.
	- For instance, if we have two random variables, X and Y, the marginal PMF of X tells us the probabilities of all possible outcomes of X regardless of the values of Y. Similarly, the marginal PMF of Y tells us the probabilities of all possible outcomes of Y regardless of the values of X.
	- Continuing with our example of rolling a die (X) and flipping a coin (Y), the marginal PMF of the die roll would give us the probabilities of rolling each number on the die, regardless of whether the coin flip resulted in heads or tails.
	- Mathematically, for a single random variable X, the marginal PMF is represented as P(X = x), where x is a specific value of X.

We use the following notations:
	- '|' is read as given, so x | Y is "x given Y"
	- The marginal PMF of X : P(X)  
	- The marginal probability of X takes the value x: p(x).  
	- Joint PMF: P(X , Y)  
	- The value of joint PMF p(X , Y) at (x, y), i.e., PXY (X = x, Y = y) : p(x, y)  
	- Conditional PMF: P(X |Y)  
	- The value of conditional PMF p(X |Y) at (x, y), i.e.,  
	- PX | Y (X = x, Y = y): p(x|y)  

#### Joint entropy
Joint entropy is a concept in information theory that quantifies the uncertainty associated with a set of variables, specifically discrete random variables X and Y. It provides a **measure of** how much **uncertainty** exists **when considering both variables simultaneously**.

The joint entropy, denoted as H(X, Y), is defined using the probability distribution of the joint occurrences of values of X and Y. Mathematically, it is represented as:

$H(X, Y) = -E[log p(X, Y)] = \sum_{x_i\in R_X} \sum_{y_j\in R_Y} p(x_i,y_j)log(p(x_i,y_j))$
Where:
- $E[]$ denotes the expected value, or the average over all possible values of X and Y.
- $p(X, Y)$ represents the joint probability mass function (PMF), which gives the probability of both X and Y taking specific values simultaneously.
- The summation is performed over all possible values $x_i$ of X and $y_j$ of Y.

In simpler terms, joint entropy calculates the average uncertainty associated with the joint occurrences of values of X and Y. It is derived from the negative of the expected value of the logarithm of the joint probability distribution. The higher the joint entropy, the greater the uncertainty or randomness associated with the combined values of X and Y.

#### Examples
Let's look at the joint entropy of two **fair** dice: 

Let's define the joint probability distribution, p(X, Y), which represents the probability of each combination of outcomes:
p(X, Y) = { (1, 1): 1/36, (1, 2): 1/36, (1, 3): 1/36, (1, 4): 1/36, (1, 5): 1/36, (1, 6): 1/36, (2, 1): 1/36, (2, 2): 1/36, (2, 3): 1/36, (2, 4): 1/36, (2, 5): 1/36, (2, 6): 1/36, (3, 1): 1/36, (3, 2): 1/36, (3, 3): 1/36, (3, 4): 1/36, (3, 5): 1/36, (3, 6): 1/36, (4, 1): 1/36, (4, 2): 1/36, (4, 3): 1/36, (4, 4): 1/36, (4, 5): 1/36, (4, 6): 1/36, (5, 1): 1/36, (5, 2): 1/36, (5, 3): 1/36, (5, 4): 1/36, (5, 5): 1/36, (5, 6): 1/36, (6, 1): 1/36, (6, 2): 1/36, (6, 3): 1/36, (6, 4): 1/36, (6, 5): 1/36, (6, 6): 1/36 }

Next, we'll calculate the joint entropy using the formula:
H(X, Y) = -Σ p(X, Y) * log2(p(X, Y))

Now, let's compute the joint entropy:
H(X, Y) = -(1/36 * log2(1/36) + 1/36 * log2(1/36) + ... + 1/36 * log2(1/36))
H(X, Y) = -(36 * (1/36) * log2(1/36))
H(X, Y) = -log2(1/36)
H(X, Y) ≈ -(-5.17)
H(X, Y) ≈ 5.17 bits

Notice that in this example, and all examples with uniform odds, we could've skipped to the stage "H(X, Y) = -log2(1/36)" for our answer, lets look at the same example with one **biased** die:

Suppose the probabilities for the biased die Y are as follows:
- P(Y = 1) = 0.1
- P(Y = 2) = 0.1
- P(Y = 3) = 0.2
- P(Y = 4) = 0.2
- P(Y = 5) = 0.2
- P(Y = 6) = 0.2

Let's define the joint probability distribution using the probabilities of each combination of outcomes:
p(X, Y) = { 
(1, 1): 0.1 * 1/6 = 1/60 
(1, 2): 0.1 * 1/6 = 1/60 
(1, 3): 0.1 * 1/6 = 1/60 
....
(6, 5): 0.2 * 1/6 = 1/30
(6, 6): 0.2 * 1/6 = 1/30 }

Now, let's calculate the joint entropy using the formula:
H(X, Y) = -Σ p(X, Y) * log2(p(X, Y))
H(X, Y) = -(1/60 * log2(1/60) + 1/60 * log2(1/60) + ... + 1/30 * log2(1/30))
H(X, Y) = -\[(36 * 1/60) * log2(1/60) + (24 * 1/30) * log2(1/30)]
H(X, Y) = -(36 * (1/60) * log2(1/60) + 24 * (1/30) * log2(1/30))
H(X, Y) = -(36 * (1/60) * (-log2(60)) + 24 * (1/30) * (-log2(30)))
H(X, Y) ≈ -(36 * (1/60) * 5.91 + 24 * (1/30) * 4.91)
H(X, Y) ≈ -(3.18 + 3.27)
H(X, Y) ≈ -6.45 bits

Notice how we can shorthand our working again, and begin from the stage: 
H(X, Y) = -(36 * (1/60) * (-log2(60)) + 24 * (1/30) * (-log2(30)))
This is only possible when the probabilities fit some sort of pattern.

#### Conditional Entropy
This quantifies uncertainty of the outcome of a random variable Y **given** the outcome of another random variable X.”, which is defined as:

$H(Y∣X)=−∑x∈X​∑y∈Y​p(x,y)⋅log2​p(y∣x)$

Denote $H(Y|X = x)$ as the entropy of the discrete random variable $( Y )$ conditioned on the discrete random variable $( X )$ taking a certain value $( x )$, then $H(Y|X)$ is the result of averaging $(Y|X = x)$ over all possible values $( x )$ that $( X )$ may take:

$H(Y|X) \equiv \sum_{x_i \in R_X} p(x_i) \cdot H(Y|X = x_i)$
$= - \sum_{x_i \in R_X} p(x_i) \sum_{y_j \in R_Y} p(y_j|x_i) \log p(y_j|x_i)$ 
$= - \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) \log p(y_j|x_i)$ 
$= -E [\log p(Y|X)]$

**Chain rule for conditional entropy** - from equation 2 and the conditional probability: $p(y|x) = \frac{p(x,y)}{p(x)}$, we have:

$H(Y|X) = - \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) \log p(y_j|x_i)$
$= - \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) \log \left( \frac{p(x_i, y_j)}{p(x_i)} \right)$
$= - \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) [\log(p(x_i, y_j)) - \log(p(x_i))]$
$= - \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) \log(p(x_i, y_j)) + \sum_{x_i \in R_X} \sum_{y_j \in R_Y} p(x_i, y_j) \log(p(x_i))$
$= H(X, Y) - H(X)$

![](../../../chrome_x9A2RoEM4p.png)

#### Relative Entropy
**Relative entropy**, or **Kullback-Leibler divergence** (KL), quantifies the distance between two probability distributions. It quantifies how one probability distribution diverges from another, and is often used in machine learning for model comparison, optimization, and regularization.

Let $P(x)$ and $Q(x)$ be two probability distribution of a discrete random variable $X$. Each of $P(x)$ and $Q(x)$ sum up to 1, and $p(x)>0$ and $q(x)>0$. For any $x \in R_X$, the KL divergence from P to Q is defined as:

$D_{KL}​(P∥Q)=∑_{x∈R_x}​P(x)log(\frac{Q(x)}{P(x)​}) = E [log\frac{P(x)}{Q(x)}]$ 

$D_{KL}​(P∥Q)$ is the KL divergence. It's like asking: "How much extra information, on average, do I need to encode the outcomes of $X$ if I use distribution $Q$ instead of distribution $P$?"

If $P(x)=Q(x)$ for all $x$, then $D_{KL}​(P∥Q)$ = 0, indicating that the distributions $P$ and $Q$ are identical. However, it is not a true distance: $D_{KL}​(P∥Q) \ne D_{KL}​(Q∥P)$.

As an example: For a binary random variable $X$ with range $R_X$ = {0, 1}, we assume two distributions $P$ and $Q$ with $P(0) = 1 − r, P(1) = r$ and $Q(0) = 1 − s, Q(1) = s$. The KL Divergences are: 

$D_{KL}(P∥Q) = (1 − r) log \frac{1 − r}{1 − s} + r log \frac{r}{s}$

$D_{KL}(Q∥P) = (1 − s) log \frac{1 − s}{1 − r} + s log \frac{s}{r}$

If $r = s$, then $D_{KL}(P∥Q) = D_{KL}(Q∥P) = 0$

If $r = \frac{1}{2}$ and $s = \frac{1}{4}$ , then:
$D_{KL}(P∥Q) = 0.2075$
$D_{KL}(Q∥P) = 0.1887$

#### Other Distance Measures
These are purely for further reading: 

**Cross Entropy**: For two discrete distributions P and Q, cross entropy is defined as: 
$H(P,Q)=−∑_{x∈R_X}​​P(x)logQ(x)=H(P)+D_{KL​}(P∥Q)$ 

**Jensen–Shannon divergence** (JSD): a symmetrized and smoothed version of the Kullback–Leibler divergence $D(P ∥ Q)$. It is defined by:
$JSD(P ∥ Q) = \frac{1}{2} D_{KL}(P ∥ M) + \frac{1}{2} D_{KL}(Q ∥ M)$
where $M =\frac{1}{2} (P + Q)$

**Wasserstein Distance** (aka. Earth Mover’s distance): the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution.

#### Mutual Information
Mutual information quantifies the degree of dependence or **shared information** between two random variables, often denoted as $X$ and $Y$. 

Intuitively, mutual information captures how much knowing one variable reduces the uncertainty about the other variable. If knowing the value of $X$ provides insights or reduces uncertainty about the possible values of $Y$, then they are likely to have high mutual information and vice versa.

Mathematically, the mutual information between X and Y, denoted as $I(X;Y)$, is defined as:

$I(X;Y) = ∑_{x∈R_X} ∑_{y∈R_Y} p(x, y) log \frac {p(x, y)} {p(x) p(y)}$

Or equivalently, it's the **KL divergence** between the joint distribution $P(X,Y)$ and the product of the marginal distributions $P(X)P(Y)$: 

$I(X;Y)=D_{KL}​(P(X,Y)∥P(X)P(Y))$

Higher mutual information indicates a stronger relationship or dependency between the variables. If $X$ and $Y$ are independent, then their mutual information is zero, indicating that knowing one variable provides no information about the other: 

i.e. if $p(x,y) = p(x)p(y)$ then $I(X;Y)=D_{KL}​(P(X,Y)∥P(X)P(Y)) = 0$

#### Further MI and KL Divergence Equivalences
Because $p(x∣y)=p(y)p(x,y)$​ we have:

$I(X;Y)=∑_{x∈R_X}​​∑_{y∈R_Y}​​p(x,y)log(\frac {p(x,y)}{p(x) p(y)​})$
$=∑_{y∈R_Y}​​p(y)∑_{x∈R_X​​}p(x∣y)log\frac{p(x∣y)}{p(x)}$
$=∑_{y∈R_Y}​​p(y)D_{KL}​(P(X∣Y)∥P(X))$
$=E_Y​[D_{KL}​(P(X∣Y)∥P(X))]$

Similarly, we have:
$I(X;Y)=E_X​[D_{KL}​(P(X∣Y)∥P(Y))]$ 

What this means is that the **mutual information** ( $I(X;Y)$ ) between the random variables X and Y is equal to the expectation, over the variable Y, of the KL divergence between the conditional distribution of X, given Y, and the marginal distribution of X.

Reminder that $E_Y$, the expected value of Y, is the average value of a function over all possible values of the random variable $Y$.

In simpler terms, it means that we're averaging the KL divergence between the distribution of $X$ conditioned on different values of $Y$ and the overall distribution of $X$ by considering all possible values of $Y$. This interpretation captures how much knowing the value of $Y$ (by taking the expectation over �Y) informs us about the distribution of $X$, which is the essence of mutual information $I(X;Y)$.

We can also express mutual information in terms of joint and conditional entropies: 

$I(X; Y ) ≡ H(X) − H(X|Y )$
$≡ H(Y ) − H(Y |X)$
$≡ H(X) + H(Y ) − H(Y , X)$
$≡ H(X, Y ) − H(Y |X) − H(X|Y )$

![](../../../chrome_0XMyWlbFxy.png)
*A visualisation of the above equations.*

#### Properties of Mutual Information
- Non-negativity: $I(X; Y ) ≥ 0$ (mutual information between $X$ and $Y$ is always greater than or equal to zero)

- Symmetric: $I(X; Y ) = I(Y ; X)$ (mutual information between $X$ and $Y$ is symmetric, meaning it doesn't matter which variable we consider as the "source" or "target" of information)

- Measure of dependence between X and Y : 
	$I(X; Y ) = 0$ iff $X ⊥ Y$, i.e. $X$ and $Y$ are independent.
	$I(X; Y )$ not only increases with the dependence of $X$ and $Y$ but also with $H(X)$ and $H(Y )$ 

- $I(X; X) = H(X) − H(X|X) = 0$ (The mutual information between $X$ and itself is equal to the entropy of $X$ minus the conditional entropy of $X$ given itself, which is always zero. This indicates that $X$ carries no additional information about itself beyond its inherent uncertainty, resulting in zero mutual information.)

#### Exercise Question 
Suppose we have a simplified language which has only three vowels: ‘a’, ‘i’ and ‘u’, and three consonants ‘p’, ‘t’ and ‘k’. Let’s denote a discrete random variable $X$ which maps the events that consonants ‘p’, ‘t’ and ‘k’ occurring to the range $R_X = \{1, 2, 3\}$. Similarly, we define the a discrete random variable $Y$ with range $R_Y = \{1, 2, 3\}$ for the occurring of vowels in the language. We estimated the joint PMF of a vowel and a consonant occurring together in the same syllable: 
![](../../../chrome_aDsZktY6l5.png)
p(x): x=1 is 1/8, x=2 is 3/4, x=3 is 1/8
p(y): y=1 is 1/2, y=2 is 1/4, y=3 is 1/4

**Entropies H(X) and H(Y)** 
H(X) = 1/8 x log_2(1/8) + 3/4 x log_2(3/4) + 1/8 x log_2(1/8) = -1.06 = 1.06 bits
H(Y) = 1/2 x log_2(1/2) + 1/4 x log_2(1/4) + 1/4 x log_2(1/4) = -1.50 = 1.50 bits

**Conditional entropies H(X|Y) and H(Y|X) and Joint entropy H(X, Y)** 
1/16 x log_2 (1/16) + 3/8 x log_2(3/8) + ... + 1/16 x log_2(1/16) = -2.44 = 2.44 bits = H(X,Y)
H(X,Y) - H(X) = H(Y|X) so 2.44 - 1.06 = 1.38
H(X,Y) - H(Y) = H(X|Y) so 2.44 - 1.50 = 0.94

**Mutual information I(X; Y)**
H(X) - H(X|Y) = I(X;Y) = 0.12
#### Decision Tree Learning and Entropy
Decision trees are popular machine learning algorithms known for their **simplicity**, **interpretability**, and **extendibility**.

Decision Tree Components:
- **Predictive Model**: Tree structure with root/internal nodes representing independent variables (features) and leaves representing outcomes of the dependent variable.
- **Types**: Classification trees (categorical dependent variables) and regression trees (continuous dependent variables).

Some popular algorithms for decision trees are: ID3, C4.5 and CART.

How should we construct a decision tree? Can we learn the structure from the data?

Given a data set, we use an algorithm that groups and labels samples that are similar between them, and looks for the best rules that split the samples that are dissimilar, until they reach certain degree of similarity. Decision tree construction involves finding rules that minimize entropy, representing uncertainty. **Gini index** and **Information gain** are key metrics for selecting optimal splits in decision tree nodes.

#### Gini Index
Gini index (used in CART): Give a training dataset of J classes, it is defined as:

$I_G(p) = 1 - ∑_{i=1}^Jp^2_i$

where $p_i$ is the fraction of items labelled with class $i$.
#### Information Gain
**Information gain** is the information we can gain after spiting the samples based on a independent variable (internal node). 

Formally, information gain is defined as the change in information entropy $H$ from a prior state to a state that takes some information as given: 

$IG(Y , X) = H(Y ) − H(Y |X)$

where $Y$ is a random variable that represent the dependent variable (labels) and $X$ is one of the independent variable, and $H(Y |X)$ is the conditional entropy of $Y$ given $X$ . 
#### Example: Cancer diagnosis
We took some biopsy of some tissues of 5 patients who are suspected to have breast cancer. We measured three biomarkers, which are either positive (T) or negative (F). 3 out of these 5 patients are confirmed as cancer by oncologists, and labelled as ‘T’. The three measures and labels are a set of training samples which are tabulated below:
![](../../../chrome_rdBzNsQqit.png)
![](../../../chrome_NFTMCLd35f.png)

We first use a random variable $Y$ to model the dependent variable (i.e., label) with the range as $R_Y = \{0, 1\}$ and 0 means normal and 1 means cancer, and each of the independent variables, i.e., the biomarkers is a random variable, denoted as $A$, $B$ and $C$, with range $R_A = R_B = R_C = \{0, 1\}$, respectively. Then we follow the following steps: 

Step 1: Calculate the entropy of random variable Y before the split
![](../../../chrome_mNJZFJScnG.png)

Step 2: Calculate the conditional entropy after a split. Since only three independent variables, we use brutal force, i.e., go over all three biomarkers:
![](../../../chrome_zTOkYr7bQY.png)

![](../../../chrome_DwljBEIsuX.png)
*Split on $A$*

Step 3: Calculate the information gain for split A:
$IG(Y , A) = H(Y ) − H(Y |A) = 0.971 − 0.951 = 0.02$

Step 4: Repeat Step 2-3 to calculate the information gain of all splits. Results:
$Split 1: IG(Y , A) = 0.02$
$Split 2: IG(Y , B) = 0.419$
$Split 3: IG(Y , C) = 0.171$

Candidate Split 2 has the highest information gain, so it will be the most favourable split for the root node.
![](../../../chrome_8LMAlftuwH.png)

#### Drawbacks of Decision Trees
**Unstable**: a small change in the data can lead to a large change in the structure of the optimal decision tree. **Relatively inaccurate**: Many other predictors such as Support Vector Machine and Neural Networks perform better than decision trees with similar data.

#### Mutual Information Feature Selection
Notice that **information gain** *is* actually **mutual information**. What this means is that decision tree learning algorithms (ID3 and C4.5) recursively use mutual information to select the independent variable that share the most information with the dependent variable, then split (make decision) the samples based on the value of this independent variable.

Given that information gain *is* mutual information, we could select a set of most relevant independent variables based on mutual information and use other powerful machine learning algorithms to build predictive models. This is called **feature selection**. We use mutual information to choose a optimal set of independent variables, called features that allow us to classify samples. 

Formally, given an initial set $F$ with n independent variables, $X = \{X_1, X_2 . . . , X_n\}$, find subset with $S ⊂ F$ features that maximises the Mutual Information $I(Y ; S)$ between the dependent variable Y (label), and the subset of selected features S.

![](../../../chrome_elXEijKtEU.png)