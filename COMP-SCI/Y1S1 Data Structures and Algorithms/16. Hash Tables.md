A **hash table** is used when we want to calculate the location of our search target, without actually searching for it. We take the input, whether that's a String, int, etc, and run it through a **hashing algorithm** to produce a **hash**.

In summary, a hash table consists of an array for storing the values, a hash function, and a mechanism for dealing with collisions. The operations are (at least): **`set(key, value)`**, **`delete(key)`**, **`lookupValue(key)`**. We could also have a method **`lookup(key)`** that returns a boolean if the key is in the hash table.

As an example, say we are trying to create a hash table for students IDs on the compsci course. These are 7-digit numbers, so we would need a very large array to store them as is, but we could `id mod 500`. This might cause **hash collisions**, where two key values have the same hash value, and so "collide". We can deal with this in two ways, the **chaining strategy**, or the **open addressing strategy**. In the open addressing strategy, if the position is occupied, we try different fallback positions. The chaining strategy stores an extra data structure on each position of the hash table, like a linked list of items that is inserted into or iterated through.

**Chaining Strategy** 
In the **chaining strategy**, entries with the same hash are stored in a **linked list**. To insert, we first check whether the key we are inserting is already in the hash table, by a linear search on the linked list at `hash(key)`. If the key is not already in the hash table, we **insert** the new key at the start of the linked list. To remove a key, we perform a **standard** linked list **deletion**

![[chrome_qAStQO2WSC.png]]

If we have a **bad hash** function that returns the same hash for many elements, then the operations are going to be **O(n)**. The **best hash function** assigns indexes to keys **uniformly**.

The **load factor** of a hash table is the average number of entries stored on a location:
$n/T$ where n is the total number of stored entries, and T is the size of the hash table. With a good hash function, an index has an **expected** length of $n/T$ entries.

On average, a linear search in a linked list of k elements takes $\frac{k+1}{2}$ comparisons. We can assert a **maximal load factor** $\lambda$, that is, $\frac{n}{T} \leq \lambda$, and guarantee a certain performance. In the Java hashtable library, this is 0.75, which means that the average key has $\leq$ 0.75 elements in the linked list.

With the assertion of a maximal load factor, $\lambda$, the average case time complexities for `lookup(key)`:
	- Unsuccessful lookup: $\frac{n}{T} \leq \lambda$ comparisons $\implies$ O(1)
	- Successful lookup: $\frac{1}{2} (1+ \frac{n}{T}) \leq \frac{1}{2} (1+\lambda)$ comparisons $\implies$ O(1)

The time complexity for `insert(key)` is the same as an unsuccessful lookup, as we have to check the linked list at the index of the hash. So time complexity for `insert(key)` is O(1).

The time complexity for `delete(key)` is the same as a successful lookup, so is O(1).

Disadvantages of the chaining strategy:
	1. Typically, there are a lot of hash collisions, therefore a lot of unused space.
	2. Linked lists require a lot of allocations, which is slow.

**Open Addressing - Linear Probing**
If the primary position at index `hash(key)` is occupied, search for the first available position to the right of it. If we reach the end, we wrap around the array to the start.

We can use `mod` to compute the "fallback" positions, where T is the size of the table:
	`hash(key)+1 mod T`, `hash(key)+2 mod T`, `hash(key)+3 mod T`, ...

For deletion, we find where the key is stored in the table, starting from the primary position and iterate to the right until we reach an empty position. If the key is stored in the table, replace it with a **tombstone** marker (marked as `#`).

For searching, we look at the `hash(key)` index and iterate to the right, skipping over any tombstone markers, until we reach an empty position, if so the element is not present in the table.

For insertion, we look at the `hash(key)` index and insert if empty, if full we search to the right, taking note of the location of the **first** tombstone we find. If we find the `key` already in the table, throw an error. If we reach an empty position, then the `key` is not currently in the table, and we insert in the noted tombstone location, if any, otherwise the empty position.

![[chrome_LF8WVZ8f2F.png]]

The time complexity of `insert`, `search` and `delete` is O(1), however we often see clustering like so:
![[chrome_YOJcxujZLd.png]]

**Primary clusters** are caused by entries with the same hash code. **Secondary clusters** are caused when the collision handling strategy causes different entries to check the same sequence of locations when they collide. Clusters are likely to get larger and larger, even if the load factor is small. To make clustering less likely, we can use **double hashing**. 

**Double Hashing**
Double hashing uses primary and secondary hash functions, `hash1(key)` and `hash2(key)`, respectively.
When inserting we try the primary position, `hash1(key)`, first and if it fails we try fallback positions:
	1. `(hash1(key) + 1*hash2(key)) mod T`
	2. `(hash1(key) + 2*hash2(key)) mod T`
	3. `(hash1(key) + 3*hash2(key)) mod T`

One problem we can encounter with this technique is **short cycling**, where the subsequent multiples of the `hash2(key)` value put us into a **loop**. This can be avoided by either making the size of the table, T, a **prime number**, or instead (and **preferably**) we make T = $2^k$ and `hash2(key)` always return an odd number.

**What is the table is full?**
We say that a hash table is **full** if the **load factor** is **more** than the **maximal load factor**. If the table becomes full after an insertion, allocate a new table **twice** the size, and insert all elements from the old table into it. 

If we double the size of the hash table, we also need to change the hash function to make sure it is *good* again. In practice, `hash(key)` is usually computed as `bigHash(key) mod T` (where `bigHash` computes a large hashcode). Then, after doubling the size of our hash table, we only modify `hash(key)` as follows: `bigHash(key) mod 2*T`.

**Summary**
Hash tables offer far better performance than say AVL trees, as they have O(1) amortized complexity for all operations, when using a *good* hash function. Double hashing has a performance advantage because the memory allocation in chaining has a large constant cost, and clustering in linear probing is worse.

In chaining, if the load factor drops below a minimum threshold, we can rehash into a table half the size, although we rarely do this as it does not affect performance.

In open addressing, we keep track of tombstones in the table, if this exceeds some threshold, we also rehash, but without doubling the size.


Can we do better than rehashing an entire table when the table is full, something to do with chaining strategy.