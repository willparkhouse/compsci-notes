**Supervised vs Unsupervised**
In **supervised learning**, we had labelled observations, where each observation is a tuple ($\textbf{x}, y$), where $\textbf{x}$ is a feature vector and y is an output label, which are related according to an unknown function f($\textbf{x}$) = y. During learning, we try to find this function that best fits observations

In **unsupervised learning**, we do not have labelled observations, but **unlabelled observations**. What insight we can gain from an unlabelled dataset is the goal of unsupervised learning. 

**Clustering**
One way we can gain insight is by finding subgroups (or **clusters**) among observations with similar traits. We are trying to find patterns in our set of feature vectors, by identifying a lower dimensional representation (**dimensionality reduction**).

There are many techniques for dimensionality reduction, like PCA, NMF, etc, but we will only focus on clustering in this module.

In unsupervised learning we don't have a simple defined goal as in supervised learning - validation of results is often **subjective**. The reasons we use unsupervised learning are that:
-**labelled datasets** are often **expensive** and **difficult** to obtain
-**dimensionality reduction** helps save on **storage** and **computation**
-we can **reduce noise**, and **irrelevant attributes** in higher dimensional data
-we don't require a preprocessing step, as in supervised learning.

**Clustering**
Find natural groupings among observations, and segment observations into clusters/groups such that:
	-Objects within a cluster have high similarity (**high intra-cluster similarity**)
	-Objects across clusters should have low similarity (**low inter-cluster similarity**)

![[chrome_J0y2xXVQI7.jpg]]

We often need to cluster based on multiple attributes, as one attribute might not work best. To do this we need some **distance functions**, we can use Euclidean, Manhattan or chebychev distances for example.

![[chrome_dvN8H3XBYr.jpg]]
*Distance functions have specific properties*

Different choice of **distance functions** yields different measures of similarity, they **implicitly assign** more **weighting** to features with large ranges than to those with small ranges. 

Rule of thumb: when no a priori domain knowledge is available, clustering should follow the principle of equal weightings to each attribute.

This necessitates normalization / data pre-processing / feature scaling of feature vectors, which ensures that attributes contribute equally to the similarity measure. We will look at **min-max normalization** and Z-**score normalization**. 

In **min-max normalization**, we rescale all values to between 0 and 1 based on the range from largest to smallest. This is sensitive to outliers.
![[chrome_Fkh2SNYayg.jpg]]

In **Z-score normalization**, all feature attributes have a mean 0 and standard deviation 1. We run
![[chrome_SDVRVaRn2m.jpg]]

*Example of Z-score*
![[Pasted image 20230427015621.png]]

After we have our normalized data we can form a **distance matrix** to compare the observations:
![[Pasted image 20230427021423.png]]

**Clustering Algorithms**
We have broadly three different types of clustering algorithms, partitional, hierarchical and model-based..
**Partitional**: Generates a single partition of the data to recover natural clusters 
Input: Feature vectors
Examples: K-means, K-medoids
**Hierarchical**: Generates a sequence of nested partitions, so clusters within clusters.
Input: Distance Matrix
Example: agglomerative clustering, divisive clustering
**Model-Based**: Assumes that data is generated **independent and identically distributed** (i.i.d.) from a mixture of distributions, each of which determines a different cluster
Example: Expectation-Maximization (EM)

**Partitional Clustering**
Goal: assign N observations in K (K<N) clusters to ensure high intra-cluster similarity and low inter-cluster similarity. We can formulate this as a combinatorial optimization problem.
![[chrome_qOqAEMWz9u.jpg]]

To attain this goal we need a measure of intra-cluster similarity - we call this variability, or inertia.
![[Pasted image 20230427023856.png]]
NOTE: we are not normalizing the data, so a cluster with more examples in the cluster will have a higher variability.

To measure the inter-cluster similarity, we use **dissimilarity**:
![[chrome_9qvOKOjT64.jpg]]

Our goal is to find a clustering structure $\textbf{C}$ of K clusters that minimizes dissimilarity - larger clusters with high variability are penalized more than smaller clusters with high variability. Under squared Euclidean distance, minimizing dissimilarity(C) is equivalent to maximizing overall inter-cluster dissimilarity (more details to come).

Finding an exact solution of the above problem is prohibitively hard:
For a number of examples 10, the number of possible assignments is ~34000
For a number of examples 19, the number of possible assignments is 10^10

To solve this, we can use iterative greedy algorithms that provide a suboptimal approximate solution, such as K-means or K-medoids.